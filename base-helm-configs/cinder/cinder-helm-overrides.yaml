---
storage: lvm

labels:
  volume:
    node_selector_key: openstack-storage-node

images:
  tags:
    bootstrap: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    cinder_api: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    cinder_backup: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    cinder_backup_storage_init: "quay.io/rackspace/rackerlabs-ceph-config-helper:latest-ubuntu_jammy"
    cinder_db_sync: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    cinder_scheduler: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    cinder_storage_init: "quay.io/rackspace/rackerlabs-ceph-config-helper:latest-ubuntu_jammy"
    cinder_volume: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    cinder_volume_usage_audit: "quay.io/rackspace/rackerlabs-cinder:2024.1-ubuntu_jammy"
    db_drop: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    db_init: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    dep_check: "quay.io/rackspace/rackerlabs-kubernetes-entrypoint:v1.0.0"
    image_repo_sync: "quay.io/rackspace/rackerlabs-docker:17.07.0"
    ks_endpoints: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    ks_service: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    ks_user: "quay.io/rackspace/rackerlabs-heat:2024.1-ubuntu_jammy"
    rabbit_init: "quay.io/rackspace/rackerlabs-rabbitmq:3.13-management"
    test: "quay.io/rackspace/rackerlabs-xrally-openstack:2.0.0"

pod:
  security_context:
    cinder_api:
      container:
        cinder_api:
          allowPrivilegeEscalation: true
          privileged: true
    cinder_backup:
      container:
        cinder_backup:
          privileged: true
    cinder_volume:
      container:
        cinder_volume:
          privileged: true
  useHostNetwork:
    backup: true
    volume: true
  oslo_db:
    host_fqdn_override:
      default: mariadb-cluster-primary.openstack.svc.cluster.local
    hosts:
      default: mariadb-cluster-primary
  oslo_cache:
    host_fqdn_override:
      default: memcached.openstack.svc.cluster.local
    hosts:
      default: memcached
  oslo_messaging:
    host_fqdn_override:
      default: rabbitmq.openstack.svc.cluster.local
    hosts:
      default: rabbitmq-nodes

conf:
  backends:
    lvmdriver-1:
      image_volume_cache_enabled: true
      iscsi_iotype: fileio
      iscsi_num_targets: 100
      lvm_type: default
      target_helper: tgtadm
      target_port: 3260
      target_protocol: iscsi
      volume_backend_name: LVM_iSCSI
      volume_clear: zero
      volume_driver: cinder_rxt.rackspace.RXTLVM
      volume_group: cinder-volumes-1
  cinder:
    DEFAULT:
      allow_availability_zone_fallback: true
      backup_compression_algorithm: zstd
      backup_swift_auth: per_user
      backup_swift_auth_version: 3
      default_availability_zone: az1
      default_volume_type: lvmdriver-1
      enabled_backends: lvmdriver-1
      osapi_volume_workers: 2
      rootwrap_config: /etc/cinder/rootwrap.conf
      scheduler_default_filters: "AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter"
      storage_availability_zone: az1
      use_multipath_for_image_xfer: false
    barbican:
      barbican_endpoint_type: internal
    key_manager:
      backend: barbican
    keystone_authtoken:
      auth_type: password
      auth_version: v3
      memcache_security_strategy: ENCRYPT
      service_token_roles: service
      service_token_roles_required: true
      service_type: volumev3
    oslo_messaging_rabbit:
      amqp_durable_queues: false
      rabbit_ha_queues: false
      rabbit_quorum_queue: true
      rabbit_transient_quorum_queue: false
      use_queue_manager: false
      rabbit_interval_max: 10
      # Send more frequent heartbeats and fail unhealthy nodes faster
      # heartbeat_timeout / heartbeat_rate / 2.0 = 30 / 3 / 2.0 = 5
      # https://opendev.org/openstack/oslo.messaging/commit/36fb5bceabe08a982ebd52e4a8f005cd26fdf6b8
      heartbeat_rate: 3
      heartbeat_timeout_threshold: 60
      # DEPRECIATION:  (warning) heartbeat_in_pthread will be deprecated in 2024.2
      heartbeat_in_pthread: True
      # Setting lower kombu_reconnect_delay should resolve issue with HA failing when one node is down
      # https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033314.html
      # https://review.opendev.org/c/openstack/oslo.messaging/+/866617
      kombu_reconnect_delay: 0.5
  cinder_api_uwsgi:
    uwsgi:
      processes: 4
      threads: 2
  enable_iscsi: true
  logging:
    logger_root:
      handlers:
        - stdout
      level: INFO
  rabbitmq:
    policies: []

dependencies:
  static:
    api:
      jobs:
        - cinder-db-sync
        - cinder-ks-user
        - cinder-ks-endpoints
    backup:
      jobs:
        - cinder-db-sync
        - cinder-ks-user
        - cinder-ks-endpoints
    db_sync:
      jobs: null
    scheduler:
      jobs:
        - cinder-db-sync
        - cinder-ks-user
        - cinder-ks-endpoints
    volume:
      jobs:
        - cinder-db-sync
        - cinder-ks-user
        - cinder-ks-endpoints
    volume_usage_audit:
      jobs:
        - cinder-db-sync
        - cinder-ks-user
        - cinder-ks-endpoints

endpoints:
  fluentd:
    namespace: fluentbit
  identity:
    port:
      api:
        default: 5000
        internal: 5000
        public: 80
        service: 5000
  image:
    port:
      api:
        default: 9292
        internal: 9292
        public: 80
        service: 9292
  oslo_db:
    host_fqdn_override:
      default: mariadb-cluster-primary.openstack.svc.cluster.local
    hosts:
      default: mariadb-cluster-primary
  oslo_cache:
    host_fqdn_override:
      default: memcached.openstack.svc.cluster.local
    hosts:
      default: memcached
  oslo_messaging:
    host_fqdn_override:
      default: rabbitmq.openstack.svc.cluster.local
    hosts:
      default: rabbitmq-nodes
  volume:
    port:
      api:
        default: 8776
        internal: 8776
        public: 80
        service: 8776
  volumev2:
    port:
      api:
        default: 8776
        internal: 8776
        public: 80
        service: 8776
  volumev3:
    port:
      api:
        default: 8776
        internal: 8776
        public: 80
        service: 8776

manifests:
  deployment_backup: false
  deployment_volume: false
  ingress_api: false
  job_bootstrap: false
  job_db_init: false
  job_rabbit_init: false
  job_storage_init: false
  pod_rally_test: false
  secret_db: false
  secret_ingress_tls: false
  secret_rabbitmq: false
  service_ingress_api: false

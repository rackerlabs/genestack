additionalPrometheusRulesMap:
  openstack-resource-alerts:
    groups:
    - name: Compute Resource Alerts
      rules:
      - alert: AbnormalInstanceFailures
        expr: count(count(last_over_time(openstack_nova_server_status{status=~"BUILD|ERROR"}[60m])) by (id)) /count(count(last_over_time(openstack_nova_server_status{status="ACTIVE"}[60m])) by (id)) * 100 >= 20
        labels:
          severity: critical
        annotations:
          summary: "Instance build failure rate is abnormally high"
          description: "This indicates a major problem building compute instances view logs and take action to resolve the build failures"
      - alert: InstancesStuckInFailureState
        expr: count(openstack_nova_server_status{status=~"BUILD|ERROR"}) > 0
        for: 90m
        labels:
          severity: warning
        annotations:
          summary: "Instances stuck in failure state for a prolonged period"
          description: "There are instance stuck in a building or error state for a prolonged period that need be cleaned up"
    - name: Image Resource Alerts
      rules:
      - alert: AbnormalImageFailures
        expr: count(count(last_over_time(openstack_glance_image_created_at{status!~"active|deactivated"}[60m])) by (id)) / count(count(last_over_time(openstack_glance_image_created_at{status="active"}[60m])) by (id)) * 100 >= 20
        labels:
          severity: critical
        annotations:
          summary: "Image create failure rate is abnormally high"
          description: "This indicates a major problem creating images view logs and take action to resolve the build failures"
      - alert: ImagesStuckInFailureState
        expr: count(openstack_glance_image_created_at{status="failure"}) > 0
        for: 90m
        labels:
          severity: warning
        annotations:
          summary: "Images stuck in failure state for a prolonged period"
          description: "There are images stuck in a failures state for a prolonged period that need be cleaned up"
    - name: Octavia Resource Alerts
      rules:
      - alert: LoadbalancersInError
        expr: count(openstack_loadbalancer_loadbalancer_status{provisioning_status="ERROR"}) > 0
        for: 90m
        labels:
          severity: critical
        annotations:
          summary: "Loadbalancer stuck in error state for a prolonged period"
          description: "This may indicate a potential problem with failover and/or healthmanager services. This could also indicate other problems building load balancers in general"
  rabbitmq-alerts:
    groups:
    - name: Prometheus Alerts
      rules:
      - alert: RabbitQueueSizeTooLarge
        expr: rabbitmq_queuesTotal>3600
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Rabbit queue size too large (instance {{ `{{ $labels.instance }}` }} )"
  database-alerts:
    groups:
    - name: Mysql Alerts
      rules:
      - alert: MysqlDown
        expr: mysql_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: MariaDB down (instance {{ $labels.instance }})
          description: "MariaDB instance is down on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: MysqlTooManyConnections(>80%)
        expr: max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Database too many connections (> 90%) (instance {{ $labels.instance }})
          description: "More than 90% of MySQL connections are in use on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: MysqlSlowQueries
        expr: increase(mysql_global_status_slow_queries[1m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: MySQL slow queries (instance {{ $labels.instance }})
          description: "MySQL server mysql has some new slow query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: MysqlRestarted
        expr: mysql_global_status_uptime < 60
        for: 0m
        labels:
          severity: info
        annotations:
          summary: MySQL restarted (instance {{ $labels.instance }})
          description: "MySQL has just been restarted, less than one minute ago on {{ $labels.instance }}.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  blackbox-alerts:
    groups:
      - name: Blackbox Alerts
        rules:
          - alert: TLS certificate expiring
            expr: (probe_ssl_earliest_cert_expiry - time())/86400 < 30
            labels:
              severity: warning
            annotations:
              summary: "SSL certificate will expire soon on (instance {{ $labels.instance }})"
              description: "SSL certificate expires within 60 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: TLS certificate expiring
            expr: (probe_ssl_earliest_cert_expiry - time())/86400 < 15
            labels:
              severity: critical
            annotations:
              summary: "SSL certificate will expire soon on (instance {{ $labels.instance }})"
              description: "SSL certificate expires within 30 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: Service Down
            expr: probe_success == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Service probe has failed for more than two minutes on (instance {{ $labels.instance }})"
              description: "Service probe has failed for more than two minutes \n  LABELS: {{ $labels }}"
  volume-alerts:
    groups:
      - name: Volume Alerts
        rules:
        - alert: KubernetesVolumeOutOfDiskSpace
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 20
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
            description: "Volume is almost full (< 20% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  backup-alerts:
    groups:
    - name: OVN backup alerts
      rules:
      - alert: ovnBackupUploadWarning
        expr: time() - upload_pairs_success_timestamp{job="ovn-backup"} > 21600 # 21600s = 6 hours
        # This bases the `expr` on the run normal interval, and allows `for`
        # to account for the amount of time to complete and upload before
        # alerting.
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: Last OVN backup not uploaded within 1 hour of scheduled run
          description: "Last OVN backup not uploaded within 1 hour of scheduled run"
      - alert: ovnBackupUploadCritical
        expr: time() - upload_pairs_success_timestamp{job="ovn-backup"} > 43200
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: Second successive of OVN backup not uploaded within 1 hour of scheduled run
          description: "Second successive of OVN backup not within 1 hour of scheduled run"
      - alert: ovnBackupDiskUsageWarning
        expr: disk_used_percent_gauge{job="ovn-backup"} > 80
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: OVN backup volume >= 80% disk usage
          description: "OVN backup volume >= 80% disk usage"
      - alert: ovnBackupDiskUsageCritical
        expr: disk_used_percent_gauge{job="ovn-backup"} > 90
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: OVN backup volume >= 90% disk usage
          description: "OVN backup volume >= 90% disk usage"
    - name: MariaDB backup alerts
      rules:
      - alert: mariadbBackupWarning
        expr: time() - kube_cronjob_status_last_successful_time{cronjob="mariadb-backup"} > 21600
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: Last MariaDB backup not successful within 1 hour of scheduled run
          description: "Last MariaDB backup not successful within 1 hour of scheduled run"
      - alert: mariadbBackupCritical
        expr: time() - kube_cronjob_status_last_successful_time{cronjob="mariadb-backup"} > 43200
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: Second successive MariaDB backup not successful within 1 hour of scheduled run
          description: "Second successive MariaDB backup not successful within 1 hour of scheduled run"
  fluentbit-serviceMonitor-alert:
    groups:
    - name: fluentbit serviceMonitor alert
      rules:
      - alert: MissingFluentbitServiceMonitor
        expr: count(up{job="fluentbit-fluent-bit"}) == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "ServiceMonitor 'fluentbit-fluent-bit' is either down or missing."
          description: "Check if the Fluentbit ServiceMonitor is properly configured and deployed."
